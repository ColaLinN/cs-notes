# 优化算法 Optimization Algorithmn

## 梯度下降（Gradient Descent, GD）

梯度下降（Gradient Descent, GD）

计算整个训练集的梯度，然后一次性更新参数。

GD 收敛稳定但慢： 由于每次更新使用全量数据，梯度方向稳定，但计算量大，收敛可能较慢。

## 随机梯度下降（Stochastic Gradient Descent, SGD）

随机梯度下降（SGD，stochastic gradient descent）。
计算单个样本的梯度，每次更新参数时只使用一个或多个数据点。

mini-batch梯度下降

每次更新使用多个数据点代表整个数据集。

## 问题

### 梯度下降和随机梯度下降对比

| 特性     | **梯度下降（GD）**           | **随机梯度下降（SGD）**          |
| -------- | ---------------------------- | -------------------------------- |
| 计算方式 | 用整个数据集计算梯度         | 用单个样本计算梯度               |
| 计算效率 | 计算成本高，适用于小数据集   | 计算成本低，适用于大数据集       |
| 收敛速度 | 收敛稳定但较慢               | 收敛快但波动大                   |
| 适用场景 | 批量数据更新，适用于小数据集 | 在线学习、深度学习、大规模数据集 |

### 随机梯度下降会收敛吗？

todo

### 常用哪些优化算法？

在深度学习和大规模数据训练中，**SGD 及其改进版本（如 Mini-batch SGD、Adam）更常用**，因为它计算更高效，适合处理海量数据。