



What's the difference between RAG, Alignment, SFT, Incremental Pretraining

> 还有那种toc 很容易被用户诱导说自家产品不好的。用prompt做约束可能不够=》这应该是要微调了=》这个微调是sft还是增量预训练。。。怎么评价？

这个讨论涉及到 LLM（大模型）在面对诱导性问题时的表现，以及如何改进其应对能力。可以从以下几个方面分析：

问题背景

TOC（Turn of Conversation）容易被用户诱导说自家产品不好，说明模型在对话控制上存在漏洞，可能是因为训练数据不足、对抗性数据覆盖不够，或 RLHF（强化学习人类反馈）未能有效约束这类情况。

用 prompt 约束可能不够，意味着仅靠提示词（Prompt Engineering）不能完全避免模型被诱导。

微调的方式

SFT（Supervised Fine-Tuning，监督微调）：即通过人工标注的数据集，对模型进行监督训练，纠正其行为。例如，可以用标注数据微调模型，让它在面对诱导性问题时学会更稳妥的回答方式。

增量预训练（Continual Pretraining）：在已有模型的基础上，继续用新的无监督数据进行训练，增强特定领域或行为模式的学习。但这种方式更适用于知识补充，而非直接纠正模型输出风格。

如何评价？

Prompt 约束不够 → 说明仅依赖 Prompt Engineering 并不能完全解决问题。

需要微调 → 这个结论是合理的，因为 SFT 可以提供更明确的行为纠正能力。

SFT vs. 增量预训练 → 在这个场景下，SFT 更适合，因为它能直接调整模型的输出，而增量预训练更适用于补充知识或调整整体语气。

## Reference

1. A Quick Introduction to LLM Alignment https://vijayasriiyer.medium.com/a-quick-introduction-to-llm-alignment-cad91d09c032
2. LLM学习记录（四）--增量预训练 https://zhuanlan.zhihu.com/p/636334904
3. 手把手教你打造本地化部署的LLM+RAG知识库（1） https://zhuanlan.zhihu.com/p/27947782392?utm_psn=1883202022423109752