



## All-reduce

如果N个GPU要通信交换批量大小为B，长度为d_model(h)的向量，其需要的时间为 2(N-1)Bh/N

ring-based-all-reduce，第一轮通信获得每个块的结果，需要 (N-1)Bh 的时间；第二次也是如此。



## All-gather

如果N个GPU要通信交换批量大小为B，长度为d_model(h)的向量，其需要的时间为 (N-1)Bh/N

ring-based-all-gather，交换需要 (N-1)Bh 的时间。