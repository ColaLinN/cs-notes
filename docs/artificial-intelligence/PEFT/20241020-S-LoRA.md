# S-LORA: SERVING THOUSANDS OF CONCURRENT LORA ADAPTERS

## 速览

核心需求

1.   如何提供微调Adapter的大规模推理服务未被讨论。
     1.   how to serve these fine-tuned variants at scale remains unexplored

2.   study how to scalably serve thousands of LoRA adapters on a single machine.

问题

1.   LoRA is often employed to adapt a base model to a multitude of tasks

挑战与解决方案

1.   GPU内存有限，考虑服务尽可能多的LoRA数量，要把LoRA放在主机内存中必要时交换到GPU显存中，容易造成内存碎片和不连续的内存，需要内存管理
     1.   Unified Paging 设计了内存池，管理LoRA和KV Cache，减少内存碎片
2.   要对不同的LoRA（不同rank的LoRA和非连续的内存）做batching很难，需要考虑新 CUDA 计算kernel
     1.   设计了新的 Heterogeneous Batching，自定义CUDA算子对非连续内存中LoRA进行计算，使之与内存池对齐
3.   单机多GPU需要设计新的tensor 并行策略，要考虑到通信和内存开销
     1.    introduces a novel tensor parallelism strategy 调度小向量的通信，融合大向量和base mode的通信

结果

1. 与  HuggingFace PEFT 吞吐量提高 30x，与 vLLM 的原生 LoRA 相比吞吐量提高了4倍，服务的Adapters数量增加了几个数量级（by several orders of magnitude. ）
2. 可以以小开销在单块 GPU 或多块 GPU 的服务器上服务数千个 LoRA adapters

评估方法

1.   Throughput, first token latency, and SLO
1.   The throughput and average request latency
1.   Ablation study comparing adapter merging and on-the-fly compute
1.   Early abort strategy experiments

## 背景

LLM 的推理过程

- 一个prompt输入到decoder中，每次处理一个token，一个prompt输入过的内容隐状态就是KV Cache（也就是使得decoding过程变得内存密集和计算密集的东西）

不同的请求的长度不同，如何batch呢？

- Orca是首个提出了细粒度的、iteration level的调度。可以在token级别batch，而不是请求级别。=> 这允许新的request不断地加入到当前运行中的batch，总的吞吐量非常高。
- vLLM在Orca的基础上提出PagedAttention优化了内存效率，基于操作系统的页表和虚拟内存管理KV Cache。 => 减少了内存碎片，增大了batch size和更高的吞吐量

LoRA 

- 一种 PEFT（Parameter-Efficient Fine-Tuning）方法，可以减少大部分fine-tuning所需参数量（比如10k倍）
- 公式如下

![image-20241020214536013](./20241020-S-LoRA.assets/image-20241020214536013.png)

内存不足、延时高怎么办？

- 模型并行

## Batching

如下图所示，本文分离了base模型和LoRA Adapter的计算

- base模型的计算由GEMM实现
- LoRA Adapters的计算由自定义的CUDA kernels实现，可以支持batching不同长度和不同秩的LoRA Adapters

![image-20241020213928913](./20241020-S-LoRA.assets/image-20241020213928913.png)

在原来的 LoRA 论文中，如下公式所示，需要先把LoRA参数和base模型参数merge，然后才能推理。如果要换到另一个adapter，需要unmerge当前的adapter再merge新的。

![image-20241020215410575](./20241020-S-LoRA.assets/image-20241020215410575.png)

如果请求都是在使用同一个LoRA相同的情况下，自然不用担心merge和unmerge带来的开销。

但在请求使用不同LoRA的场景下，merge和unmerge的开销就很明显了。除非同时运行多个LoRA merge后的不同模型的实例，不然不能同时服务不同类的请求，而多实例也会带来巨大的内存开销。

在本文中，如下公式所示，把LoRA和base model的计算分离，实时serving。相较于原方法，虽然会带来一些额外的内存和计算开销（xAB），但其开销远小于base模型计算（xW）的开销。

![image-20241020215501243](./20241020-S-LoRA.assets/image-20241020215501243.png)

在实现这样的方法时，由于Batching中LoRA的秩和请求seq_len都不同，现有的GEMM kernel会带来大量的padding操作，所以本文提出了自定义的CUDA kernel来避免padding、优化效率。

本文基于Ocra中提出的token level的batching，一个请求会在内存允许的情况下立马被服务，只有在输出了结束token（结束标志）或达到最大生成长度时才会退出batching。

## Unified Paging

如下图所示，Adapters主要存在内存中，在需要时才会被加载到GPU中（这样子可以腾出空间给KV Cache 和模型参数，允许更大的请求batch），所以可以服务多少个Adapters取决于内存有多大。

- Adapter Clustering：可以prioritize使用相同Adapter的请求，把需要的Adapter加载到GPU内存中
  - 会影响吞吐量Throughput和Latency
- Admission control strategy：服务有SLO指标，如果收到过量的请求需要drop。
  - 本文有early abort strategy去判断是否Drop这个请求，避免服务器达不到SLO。
  - 本文也会预测请求的类型，提前去调度所需LoRA

![image-20241020221545072](./20241020-S-LoRA.assets/image-20241020221545072.png)

挑战

1. 动态加载和卸载LoRA Adapters带来内存碎片
2. 为了加载LoRA的延时影响了请求的延时，需要预测接下来要使用的Adapter，然后预加载（要把输入输出和计算并行）

KV Cache 和 LoRA Adapter 的相似性

1. 大小：KV Cache的大小与请求的seq_len有关，LoRA Adapter的秩r大小与请求类型有关
2. 加载和卸载：KV Cache在推理时生成，在推理完成后清空；LoRA则在需要推理时加载，推理完成后可以卸载。都会带来内存碎片问题
3. 维度：KV Cache的维度有是(seq_len, H), 而LoRA的维度是(r, H)，都是H(n_hidden)

基于上面的相似性，本文设计了Unified Memory Pool

- 每页的大小(1, H)。在这个pool中，seq_len为S的KV Cache会使用S页，秩为R的LoRA会使用R页。都占满了整页，减少了内存碎片。
- 所以KV Cache和LoRA都是非连续、交错地存储在内存池中的。

![image-20241020224837325](./20241020-S-LoRA.assets/image-20241020224837325.png)

## Customized CUDA kernel

综上所述，由于Adapters的秩不同，且存在于上述的不连续的内存页面中。需要自定义CUDA kernels，以支持Batching LoRA Adapters的计算。

有两个阶段

1. pre-fill阶段。
   1. 需求：kernel需要处理tokens、从内存池中gather不同秩的adapter weights。
   2. 本文使用了Multi-size Batched Gather Matrix-Matrix Multiplication(MBGMM)。在 Triton系统中通过平铺tiling实现。
2. decode阶段
   1. 需求：kernel需要处理一个tokens、从内存池中gather不同秩的adapter weights。
   2. 称这个kernel为Multi-size Batched Gather Matrix-Vector Multiplication (MBGMV)。本文实现了Triton和Punica两个版本，Punica提出的kernel支持不同秩的LoRA、不连续的内存、更细粒度的内存gathering。

这里提到了两个kernel，来自triton和Punica。Punica是2023年的工作，支持服务不同的LoRA

GEMM (NVIDIA) that can be used for heterogeneous batching.

## Tensor Parallelism

在LLM推理中，模型并行最常见，因为模型并行只需要一份程序，只要分割模型即可，可以降低每个GPU的延时和内存需求。

由于LoRA引入了新的参数和矩阵乘法，所以需要设计模型并行的方案。

- Base model 的并行策略是 Megatron-LM 的模型并行策略。
- 要使得Added LoRA的计算对齐Base Model输入输出的划分（Partition）

![image-20241021013817220](./20241020-S-LoRA.assets/image-20241021013817220.png)

在上图中，以FFN的两个全连接层为例

1. 维度（dimension）
   1. B是输入的token数目
   2. h是input维度
   3. N是GPU数
   4. d是模型hidden维度
   5. r是LoRA adapter的秩
2. 原计算公式与划分（partition）
   1. `x1=xW1+xA1B1`。对W1做col-partition，对A1做col-partition，对B1做row-partition
   2. `x2=xW2+xA2B2`。对W2做row-partition，对A2做row-partition，对B2做col-partition
3. 划分（partition）规则：col-partition完后维度会变成`(*，*/N)`，所以要对后面要乘的矩阵做row-partition `(*/N, *)`

创新。在最后的matmul_4中，本应all-gather收集到完整的xAB结果、与matmul_2结果相加得到add_2，最后all-reduce得到最终的结果。S-LoRA把这个all-gather和all-reduce融合起来做了。

最终的S-LoRA实现。与 Megatron-LM 的模型并行类似

- 对self-attention层的head dimension做partition（即query-key-value的映射对应下图中的W1，后面的combination乘法对应W2）
- 对FFN层做partition。

通信量。因为 `r<<h`，所以base-model的通信量 `2(N-1)Bh/N` 远远大于LoRA计算引入的通信量`5(N-1)Br/N

![image-20241021024434663](./20241020-S-LoRA.assets/image-20241021024434663.png)



![image-20241021025516534](./20241020-S-LoRA.assets/image-20241021025516534.png)

## Evaluation



## 相关经典论文归纳

1. Ocra 做token batching的请求调度
2. LoRA 低秩适配
3. Megtrogan-LM 2019，模型并行
4. Triton和Punica，做推理优化
5. GEMM 操作 by CUDA

## Questions

1. 所以 LoRA 是应用到 attention 层的还是 FFN？
   1. Typically, this adjustment is only applied to the query, key, value, and output projection matrices in the self-attention module, excluding the feed-forward module.
   2. 但网上有人手撕里面写的是在FFN
2. 请求的Batching
3. 