# PEFT - LoRA

参数高效微调（Parameter-Efficient Fine-Tune，PEFT) 

# Todolist

- [x] LoRA: Low-Rank Adaptation of Large Language Models https://arxiv.org/abs/2106.09685
  - [x] 降低模型超参数调优的成本
  - [ ] 精读

- [x] dLoRA: Dynamically Orchestrating Requests and Adapters for LoRA LLM Serving https://www.usenix.org/conference/osdi24/presentation/wu-bingyang
  - [x] 动态调度request和adapter（merge/unmerge），提高GPU使用率和请求吞吐量
  - [ ] 精读

- [x] Fairness in Serving Large Language Models https://www.usenix.org/conference/osdi24/presentation/sheng
  - [x] 公平的负载均衡，确保GPU利用率。确保低负载用户请求被满足（不starve），高负载用户用户在GPU可用的情况下可以被尽可能满足。
  - [ ] 精读

- [ ] SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models https://arxiv.org/pdf/2308.06522

## LoRA（Low-Rank Adaptation）

问题：

1.   大模型 full fine-tuning 需要训练所有的参数，不可行。比如要部署GPT3 175B 模型实例成本非常高

解决方案

1.   冻结预训练模型的权重，将可训练的低秩分解矩阵注入模型的每一层
     1.   在LORA的策略下，增加了右侧的“旁支”，也就是先用一个Linear层A，将数据从d dd维降到r，这个r也就是LORA的秩，是LORA中最重要的一个超参数。一般会远远小于d

2.   大大减少可以训练的参数量

效果

1.   与使用 Adam fine-tune 的 GPT-3 175B 模型相比，LoRA 能够减少 10000X 的可训练参数，减少 3 倍的 GPU 内存需求。
2.   与 adapters 不同，LoRA 的需要训练的参数少，并且没有额外的推理延时。

开源代码 https://github.com/microsoft/LoRA

论文 https://arxiv.org/abs/2106.09685

![image-20241004110526462](20240928-peft.assets/image-20241004110526462.png)

缺点

1.   无法在单 batch 进行多个不同任务的处理。
     1.   多个 LoRA 没法同时merge到一个模型上，模型加载到显存后只能对一个 batch 加载一个 LoRA 参数，对于云端部署推理的时候，对于多任务部署时可能 batch 没法打满，减缓推理速度。
2.   不具备非线性能力
     1.   使用 LoRA 时需要将 BA 矩阵 merge 回原始模型，在尝试对 BA 中间加入非线性变换后，发现整个结果变得很差

## dLoRA

-   问题
    -   GPU 使用率很低，不同类型的请求与 LoRA merge/unmegre 的时机没有很好的调度

-   方法
    -   dLoRA 可以通过动态的调度 requests and LoRA adapters 实现高效的推理性能
        1. 基于 credit 的 batching 算法决定何时 merge、unmerge，动态地从 base model 中 merge/unmerge LoRa adapters
        2. request-adapter co-migration algorithm 决定何时 mirgate ，动态地在不同的worker实例间调度 migrate 请求与adapters

-   experimental results show that dLoRA improves the throughput by up to 57.9× and 26.0×, compared to vLLM and HugginFace PEFT.

![image-20240928131602414](20240928-peft.assets/image-20240928131602414.png)

![image-20240928131642446](20240928-peft.assets/image-20240928131642446.png)

## Fairness in Serving Large Language Models

OSDI '24

问题

1. 服务的负载有限
2. 以前的研究只考虑了性能，但是没有考虑公平
   1. 不同用户的请求量，请求的 load 不同，如过只有请求级别负载均衡（如FCFS），那么请求量小的用户会比请求量大的用户获得执行的机率少，

![image-20241005132112642](./20240928-peft.assets/image-20241005132112642.png)

现有的解决方案

1. 平均 rate limit，但 rate limit 在服务器负载未达到极限的情况下会导致请求量大的用户无法被满足

![image-20241005132254127](./20240928-peft.assets/image-20241005132254127.png)

目标

1. GPU资源不被浪费
2. 请求量大，load大的用户可以在GPU资源有限的情况

![image-20241005133009939](./20240928-peft.assets/image-20241005133009939.png)

评估方式

![image-20241005132942764](./20240928-peft.assets/image-20241005132942764.png)

## Reference

1.   LoRA: Low-Rank Adaptation of Large Language Models https://arxiv.org/abs/2106.09685
2.   dLoRA: Dynamically Orchestrating Requests and Adapters for LoRA LLM Serving https://www.usenix.org/conference/osdi24/presentation/wu-bingyang
3.   Fairness in Serving Large Language Models https://www.usenix.org/conference/osdi24/presentation/sheng
4.   SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models https://arxiv.org/pdf/2308.06522
5.   [【LLM系列 | PEFT】*什么是LoRA*？](https://zhuanlan.zhihu.com/p/673114053)
6.   [PEFT与LORA介绍](https://cloud.baidu.com/qianfandev/topic/268731)
